{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Подключаем пакеты и определяем функции"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Для работы с данными\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import wfdb\n",
    "import ast\n",
    "from utils import utils\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt   # plotting\n",
    "#import seaborn as sns   # plotting heatmap\n",
    "\n",
    "# Для работы с моделями\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras import layers\n",
    "\n",
    "# Для метрик\n",
    "from keras import backend as K\n",
    "from keras.metrics import AUC, Recall, Precision, Accuracy, TruePositives, TrueNegatives, FalsePositives, FalseNegatives\n",
    "from sklearn.metrics import fbeta_score, precision_score, recall_score, accuracy_score, roc_auc_score\n",
    "\n",
    "# Функции\n",
    "# Компиляция и обучение модели\n",
    "def compile_fit(model, X_train, y_train, X_val, y_val, early_stopping, model_checkpoint):\n",
    "  model.compile(loss = keras.losses.CategoricalCrossentropy(),\n",
    "                optimizer=tf.optimizers.Adam(),\n",
    "                metrics=['accuracy'])\n",
    "\n",
    "  history = model.fit(X_train, y_train, epochs = 30, validation_data = (X_val, y_val), callbacks=[model_checkpoint, early_stopping])\n",
    "  return history\n",
    "\n",
    "tf.random.set_seed(42)\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Скачиваем ICBEB с использованием кода и обработки авторов исследуемой статьи (обработанные данные, т.е. проведена нормализация и категоризация)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((3337, 1000, 12), (3337, 4), (378, 1000, 12), (378, 4))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampling_frequency=100\n",
    "datafolder='data/ICBEB/'\n",
    "task='diagnostic'\n",
    "#task='superdiagnostic'\n",
    "#task = 'subdiagnostic'\n",
    "#task = 'rhythm'\n",
    "#task = 'form'\n",
    "outputfolder='output/'\n",
    "\n",
    "# Load ICBEB data\n",
    "data, raw_labels = utils.load_dataset(datafolder, sampling_frequency)\n",
    "# Preprocess label data\n",
    "labels = utils.compute_label_aggregations(raw_labels, datafolder, task)\n",
    "# Select relevant data and convert to one-hot\n",
    "data, labels, Y, _ = utils.select_data(data, labels, task, min_samples=0, outputfolder=outputfolder)\n",
    "\n",
    "# 1-9 for training \n",
    "X_train = data[labels.strat_fold < 10]\n",
    "y_train = Y[labels.strat_fold < 10]\n",
    "# 10 for validation\n",
    "X_val = data[labels.strat_fold == 10]\n",
    "y_val = Y[labels.strat_fold == 10]\n",
    "\n",
    "# Стандартизация 3D данных c применением StandardScaler.\n",
    "# Сначала изменяется форма данных а затем применяется нормализация. После этого требуется вернуть их прежнюю форму \n",
    "def standard_scaler(X_train, X_val): \n",
    "  scaler = StandardScaler()\n",
    "  # Train\n",
    "  num_instances, num_time_steps, num_features = X_train.shape\n",
    "  X_train = np.reshape(X_train, newshape=(-1, num_features))\n",
    "  X_train = scaler.fit_transform(X_train)\n",
    "  X_train = np.reshape(X_train, newshape=(num_instances, num_time_steps, num_features))\n",
    "  # Valid\n",
    "  num_instances, num_time_steps, num_features = X_val.shape\n",
    "  X_val = np.reshape(X_val, newshape=(-1, num_features))\n",
    "  X_val = scaler.fit_transform(X_val)\n",
    "  X_val = np.reshape(X_val, newshape=(num_instances, num_time_steps, num_features))\n",
    "  return X_train, X_val\n",
    "\n",
    "X_train.shape, y_train.shape, X_val.shape, y_val.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### task = 'diagnostic'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Выделим уникальные значения y_train в столбце и посчитаем их количество\n",
    "# unique, counts = np.unique(y_train, return_counts = True, axis = 0) \n",
    "# print(unique, '\\n', 'len(unique): ', len(unique))\n",
    "# print(counts, '\\n', 'len(counts): ', len(counts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Стандартизация данных\n",
    "X_train, X_val = standard_scaler(X_train, X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Сохранение наборов данных в файлы .npy для дальнейшего использования в Google Colab\n",
    "np.save('X_train_ICBEB_diag', X_train)\n",
    "np.save('X_val_ICBEB_diag', X_val)\n",
    "np.save('y_train_ICBEB_diag', y_train)\n",
    "np.save('y_val_ICBEB_diag', y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### task = 'superdiagnostic'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Выделим уникальные значения y_train в столбце и посчитаем их количество\n",
    "# unique, counts = np.unique(y_train, return_counts = True, axis = 0) \n",
    "# print(unique, '\\n', 'len(unique): ', len(unique))\n",
    "# print(counts, '\\n', 'len(counts): ', len(counts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Стандартизация данных\n",
    "X_train, X_val = standard_scaler(X_train, X_val)\n",
    "\n",
    "# Сохранение наборов данных в файлы .npy для дальнейшего использования в Google Colab\n",
    "np.save('X_train_ICBEB_superdiag', X_train)\n",
    "np.save('X_val_ICBEB_superdiag', X_val)\n",
    "np.save('y_train_ICBEB_superdiag', y_train)\n",
    "np.save('y_val_ICBEB_superdiag', y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### task = 'subdiagnostic'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Выделим уникальные значения y_train в столбце и посчитаем их количество\n",
    "# unique, counts = np.unique(y_train, return_counts = True, axis = 0) \n",
    "# print(unique, '\\n', 'len(unique): ', len(unique))\n",
    "# print(counts, '\\n', 'len(counts): ', len(counts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Стандартизация данных\n",
    "X_train, X_val = standard_scaler(X_train, X_val)\n",
    "\n",
    "# Сохранение наборов данных в файлы .npy для дальнейшего использования в Google Colab\n",
    "np.save('X_train_ICBEB_subdiag', X_train)\n",
    "np.save('X_val_ICBEB_subdiag', X_val)\n",
    "np.save('y_train_ICBEB_subdiag', y_train)\n",
    "np.save('y_val_ICBEB_subdiag', y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### task = 'rhythm'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Выделим уникальные значения y_train в столбце и посчитаем их количество\n",
    "# unique, counts = np.unique(y_train, return_counts = True, axis = 0) \n",
    "# print(unique, '\\n', 'len(unique): ', len(unique))\n",
    "# print(counts, '\\n', 'len(counts): ', len(counts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Стандартизация данных\n",
    "X_train, X_val = standard_scaler(X_train, X_val)\n",
    "\n",
    "# Сохранение наборов данных в файлы .npy для дальнейшего использования в Google Colab\n",
    "np.save('X_train_ICBEB_rhythm', X_train)\n",
    "np.save('X_val_ICBEB_rhythm', X_val)\n",
    "np.save('y_train_ICBEB_rhythm', y_train)\n",
    "np.save('y_val_ICBEB_rhythm', y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### task = 'form'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Выделим уникальные значения y_train в столбце и посчитаем их количество\n",
    "# unique, counts = np.unique(y_train, return_counts = True, axis = 0) \n",
    "# print(unique, '\\n', 'len(unique): ', len(unique))\n",
    "# print(counts, '\\n', 'len(counts): ', len(counts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Стандартизация данных\n",
    "X_train, X_val = standard_scaler(X_train, X_val)\n",
    "\n",
    "# Сохранение наборов данных в файлы .npy для дальнейшего использования в Google Colab\n",
    "np.save('X_train_ICBEB_form', X_train)\n",
    "np.save('X_val_ICBEB_form', X_val)\n",
    "np.save('y_train_ICBEB_form', y_train)\n",
    "np.save('y_val_ICBEB_form', y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Скачиваем PTB-XL с использованием кода и обработки авторов исследуемой статьи (обработанные данные, т.е. проведена нормализация и категоризация)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((8106, 1000, 12), (8106, 19), (882, 1000, 12), (882, 19))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampling_frequency=100\n",
    "datafolder='data/ptbxl/'\n",
    "#task='diagnostic'\n",
    "#task='superdiagnostic'\n",
    "#task = 'subdiagnostic'\n",
    "#task = 'rhythm'\n",
    "task = 'form'\n",
    "outputfolder='output/'\n",
    "\n",
    "# Load PTB-XL data\n",
    "data, raw_labels = utils.load_dataset(datafolder, sampling_frequency)\n",
    "# Preprocess label data\n",
    "labels = utils.compute_label_aggregations(raw_labels, datafolder, task)\n",
    "# Select relevant data and convert to one-hot\n",
    "data, labels, Y, _ = utils.select_data(data, labels, task, min_samples=0, outputfolder=outputfolder)\n",
    "\n",
    "# 1-9 for training \n",
    "X_train = data[labels.strat_fold < 10]\n",
    "y_train = Y[labels.strat_fold < 10]\n",
    "# 10 for validation\n",
    "X_val = data[labels.strat_fold == 10]\n",
    "y_val = Y[labels.strat_fold == 10]\n",
    "\n",
    "# Стандартизация 3D данных c применением StandardScaler.\n",
    "# Сначала изменяется форма данных а затем применяется нормализация. После этого требуется вернуть их прежнюю форму \n",
    "def standard_scaler(X_train, X_val): \n",
    "  scaler = StandardScaler()\n",
    "  # Train\n",
    "  num_instances, num_time_steps, num_features = X_train.shape\n",
    "  X_train = np.reshape(X_train, newshape=(-1, num_features))\n",
    "  X_train = scaler.fit_transform(X_train)\n",
    "  X_train = np.reshape(X_train, newshape=(num_instances, num_time_steps, num_features))\n",
    "  # Valid\n",
    "  num_instances, num_time_steps, num_features = X_val.shape\n",
    "  X_val = np.reshape(X_val, newshape=(-1, num_features))\n",
    "  X_val = scaler.fit_transform(X_val)\n",
    "  X_val = np.reshape(X_val, newshape=(num_instances, num_time_steps, num_features))\n",
    "  return X_train, X_val\n",
    "\n",
    "X_train.shape, y_train.shape, X_val.shape, y_val.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### task = 'diagnostic'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Выделим уникальные значения y_train в столбце и посчитаем их количество\n",
    "# unique, counts = np.unique(y_train, return_counts = True, axis = 0) \n",
    "# print(unique, '\\n', 'len(unique): ', len(unique))\n",
    "# print(counts, '\\n', 'len(counts): ', len(counts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Стандартизация данных\n",
    "X_train, X_val = standard_scaler(X_train, X_val)\n",
    "\n",
    "# Сохранение наборов данных в файлы .npy для дальнейшего использования в Google Colab\n",
    "np.save('X_train_ptbxl_diag', X_train)\n",
    "np.save('X_val_ptbxl_diag', X_val)\n",
    "np.save('y_train_ptbxl_diag', y_train)\n",
    "np.save('y_val_ptbxl_diag', y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### task = 'superdiagnostic'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Выделим уникальные значения y_train в столбце и посчитаем их количество\n",
    "# unique, counts = np.unique(y_train, return_counts = True, axis = 0) \n",
    "# print(unique, '\\n', 'len(unique): ', len(unique))\n",
    "# print(counts, '\\n', 'len(counts): ', len(counts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Стандартизация данных\n",
    "X_train, X_val = standard_scaler(X_train, X_val)\n",
    "\n",
    "# Сохранение наборов данных в файлы .npy для дальнейшего использования в Google Colab\n",
    "np.save('X_train_ptbxl_superdiag', X_train)\n",
    "np.save('X_val_ptbxl_superdiag', X_val)\n",
    "np.save('y_train_ptbxl_superdiag', y_train)\n",
    "np.save('y_val_ptbxl_superdiag', y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### task = 'subdiagnostic'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Выделим уникальные значения y_train в столбце и посчитаем их количество\n",
    "# unique, counts = np.unique(y_train, return_counts = True, axis = 0) \n",
    "# print(unique, '\\n', 'len(unique): ', len(unique))\n",
    "# print(counts, '\\n', 'len(counts): ', len(counts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Стандартизация данных\n",
    "X_train, X_val = standard_scaler(X_train, X_val)\n",
    "\n",
    "# Сохранение наборов данных в файлы .npy для дальнейшего использования в Google Colab\n",
    "np.save('X_train_ptbxl_subdiag', X_train)\n",
    "np.save('X_val_ptbxl_subdiag', X_val)\n",
    "np.save('y_train_ptbxl_subdiag', y_train)\n",
    "np.save('y_val_ptbxl_subdiag', y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### task = 'rhythm'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Выделим уникальные значения y_train в столбце и посчитаем их количество\n",
    "# unique, counts = np.unique(y_train, return_counts = True, axis = 0) \n",
    "# print(unique, '\\n', 'len(unique): ', len(unique))\n",
    "# print(counts, '\\n', 'len(counts): ', len(counts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Стандартизация данных\n",
    "X_train, X_val = standard_scaler(X_train, X_val)\n",
    "\n",
    "# Сохранение наборов данных в файлы .npy для дальнейшего использования в Google Colab\n",
    "np.save('X_train_ptbxl_rhythm', X_train)\n",
    "np.save('X_val_ptbxl_rhythm', X_val)\n",
    "np.save('y_train_ptbxl_rhythm', y_train)\n",
    "np.save('y_val_ptbxl_rhythm', y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### task = 'form'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Выделим уникальные значения y_train в столбце и посчитаем их количество\n",
    "# unique, counts = np.unique(y_train, return_counts = True, axis = 0) \n",
    "# print(unique, '\\n', 'len(unique): ', len(unique))\n",
    "# print(counts, '\\n', 'len(counts): ', len(counts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Стандартизация данных\n",
    "X_train, X_val = standard_scaler(X_train, X_val)\n",
    "\n",
    "# Сохранение наборов данных в файлы .npy для дальнейшего использования в Google Colab\n",
    "np.save('X_train_ptbxl_form', X_train)\n",
    "np.save('X_val_ptbxl_form', X_val)\n",
    "np.save('y_train_ptbxl_form', y_train)\n",
    "np.save('y_val_ptbxl_form', y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Скачиваем ICBEB через форму PTB-XL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\manuk\\AppData\\Local\\Temp\\ipykernel_7528\\110442798.py:7: DeprecationWarning: elementwise comparison failed; this will raise an error in the future.\n",
      "  data.remove(value)\n",
      "C:\\Users\\manuk\\AppData\\Local\\Temp\\ipykernel_7528\\110442798.py:7: DeprecationWarning: elementwise comparison failed; this will raise an error in the future.\n",
      "  data.remove(value)\n",
      "C:\\Users\\manuk\\AppData\\Local\\Temp\\ipykernel_7528\\110442798.py:7: DeprecationWarning: elementwise comparison failed; this will raise an error in the future.\n",
      "  data.remove(value)\n",
      "C:\\Users\\manuk\\AppData\\Local\\Temp\\ipykernel_7528\\110442798.py:7: DeprecationWarning: elementwise comparison failed; this will raise an error in the future.\n",
      "  data.remove(value)\n",
      "C:\\Users\\manuk\\AppData\\Local\\Temp\\ipykernel_7528\\110442798.py:7: DeprecationWarning: elementwise comparison failed; this will raise an error in the future.\n",
      "  data.remove(value)\n",
      "C:\\Users\\manuk\\AppData\\Local\\Temp\\ipykernel_7528\\110442798.py:7: DeprecationWarning: elementwise comparison failed; this will raise an error in the future.\n",
      "  data.remove(value)\n"
     ]
    }
   ],
   "source": [
    "def load_raw_data(df, sampling_rate, path):\n",
    "    if sampling_rate == 100:\n",
    "        data = [wfdb.rdsamp(path+f) for f in df.filename]\n",
    "    data = [signal for signal, meta in data]\n",
    "    for index, value in enumerate(data):\n",
    "      if len(value) < 1000:\n",
    "        data.remove(value)\n",
    "        df = df.drop(index = index)\n",
    "    refLen = 1000 # reference - эталон\n",
    "    for index, value in enumerate(data):\n",
    "      if len(value) > 1000:\n",
    "        data[index] = data[index][:refLen]\n",
    "    data = np.array(data)\n",
    "    return data, df\n",
    "\n",
    "path = 'data/ICBEB/'\n",
    "sampling_rate=100\n",
    "\n",
    "# load and convert annotation data\n",
    "Y = pd.read_csv(path+'icbeb_database.csv', index_col='ecg_id')\n",
    "Y.scp_codes = Y.scp_codes.apply(lambda x: ast.literal_eval(x))\n",
    "\n",
    "# Load raw signal data\n",
    "X, Y = load_raw_data(Y, sampling_rate, path)\n",
    "\n",
    "# Load scp_statements.csv for diagnostic aggregation\n",
    "agg_df = pd.read_csv(path+'scp_statements.csv', index_col=0)\n",
    "agg_df = agg_df[agg_df.form == 1]\n",
    "\n",
    "def aggregate_diagnostic(y_dic):\n",
    "    tmp = []\n",
    "    for key in y_dic.keys():\n",
    "        if key in agg_df.index:\n",
    "            tmp.append(agg_df.loc[key].diagnostic_class)\n",
    "    return list(set(tmp))\n",
    "\n",
    "# Apply diagnostic superclass\n",
    "Y['diagnostic_superclass'] = Y.scp_codes.apply(aggregate_diagnostic)\n",
    "\n",
    "# # Split data into train and test\n",
    "test_fold = 10\n",
    "# Train\n",
    "X_train = X[np.where(Y.strat_fold != test_fold)]\n",
    "y_train = Y[(Y.strat_fold != test_fold)].diagnostic_superclass\n",
    "# Test\n",
    "X_test = X[np.where(Y.strat_fold == test_fold)]\n",
    "y_test = Y[Y.strat_fold == test_fold].diagnostic_superclass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((6181, 1000, 12), (6181,))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.02822609,  0.00672717, -0.02150135, ..., -0.11200486,\n",
       "        -0.59595824, -0.01558333],\n",
       "       [ 0.05623843,  0.02373605, -0.03248586, ..., -0.09599664,\n",
       "        -0.55091871,  0.03543248],\n",
       "       [ 0.06525131,  0.05676969, -0.00847507, ..., -0.05695029,\n",
       "        -0.48688198,  0.10148025],\n",
       "       ...,\n",
       "       [ 0.19612203, -0.16015955, -0.35627225, ...,  0.38222251,\n",
       "         0.2396717 , -0.25792291],\n",
       "       [-0.01297901, -0.48225593, -0.46927696, ..., -0.44072623,\n",
       "        -0.56097927, -0.85331045],\n",
       "       [-0.34466227, -0.59409148, -0.24942707, ..., -0.98916564,\n",
       "        -1.00918153, -1.11767564]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ecg_id\n",
       "1          []\n",
       "2          []\n",
       "3          []\n",
       "5          []\n",
       "6          []\n",
       "        ...  \n",
       "6873       []\n",
       "6874    [nan]\n",
       "6875       []\n",
       "6876       []\n",
       "6877       []\n",
       "Name: diagnostic_superclass, Length: 6181, dtype: object"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>validation</th>\n",
       "      <th>age</th>\n",
       "      <th>sex</th>\n",
       "      <th>scp_codes</th>\n",
       "      <th>patient_id</th>\n",
       "      <th>quality</th>\n",
       "      <th>strat_fold</th>\n",
       "      <th>diagnostic_superclass</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ecg_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>records100/1</td>\n",
       "      <td>False</td>\n",
       "      <td>74.0</td>\n",
       "      <td>1</td>\n",
       "      <td>{'CRBBB': 100}</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>records100/2</td>\n",
       "      <td>False</td>\n",
       "      <td>49.0</td>\n",
       "      <td>0</td>\n",
       "      <td>{'NORM': 100}</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>records100/3</td>\n",
       "      <td>False</td>\n",
       "      <td>81.0</td>\n",
       "      <td>0</td>\n",
       "      <td>{'AFIB': 100}</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>records100/4</td>\n",
       "      <td>False</td>\n",
       "      <td>45.0</td>\n",
       "      <td>1</td>\n",
       "      <td>{'AFIB': 100}</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>records100/5</td>\n",
       "      <td>False</td>\n",
       "      <td>53.0</td>\n",
       "      <td>1</td>\n",
       "      <td>{'VPC': 100}</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6873</th>\n",
       "      <td>records100/6873</td>\n",
       "      <td>False</td>\n",
       "      <td>80.0</td>\n",
       "      <td>1</td>\n",
       "      <td>{'1AVB': 100}</td>\n",
       "      <td>6873</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6874</th>\n",
       "      <td>records100/6874</td>\n",
       "      <td>False</td>\n",
       "      <td>62.0</td>\n",
       "      <td>0</td>\n",
       "      <td>{'STD_': 100}</td>\n",
       "      <td>6874</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>[nan]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6875</th>\n",
       "      <td>records100/6875</td>\n",
       "      <td>False</td>\n",
       "      <td>78.0</td>\n",
       "      <td>1</td>\n",
       "      <td>{'CLBBB': 100}</td>\n",
       "      <td>6875</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6876</th>\n",
       "      <td>records100/6876</td>\n",
       "      <td>False</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>{'AFIB': 100}</td>\n",
       "      <td>6876</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6877</th>\n",
       "      <td>records100/6877</td>\n",
       "      <td>False</td>\n",
       "      <td>71.0</td>\n",
       "      <td>0</td>\n",
       "      <td>{'VPC': 100}</td>\n",
       "      <td>6877</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6871 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               filename  validation   age  sex       scp_codes  patient_id  \\\n",
       "ecg_id                                                                       \n",
       "1          records100/1       False  74.0    1  {'CRBBB': 100}           1   \n",
       "2          records100/2       False  49.0    0   {'NORM': 100}           2   \n",
       "3          records100/3       False  81.0    0   {'AFIB': 100}           3   \n",
       "4          records100/4       False  45.0    1   {'AFIB': 100}           4   \n",
       "5          records100/5       False  53.0    1    {'VPC': 100}           5   \n",
       "...                 ...         ...   ...  ...             ...         ...   \n",
       "6873    records100/6873       False  80.0    1   {'1AVB': 100}        6873   \n",
       "6874    records100/6874       False  62.0    0   {'STD_': 100}        6874   \n",
       "6875    records100/6875       False  78.0    1  {'CLBBB': 100}        6875   \n",
       "6876    records100/6876       False  -1.0    0   {'AFIB': 100}        6876   \n",
       "6877    records100/6877       False  71.0    0    {'VPC': 100}        6877   \n",
       "\n",
       "        quality  strat_fold diagnostic_superclass  \n",
       "ecg_id                                             \n",
       "1             0           7                    []  \n",
       "2             0           7                    []  \n",
       "3             0           3                    []  \n",
       "4             0          10                    []  \n",
       "5             0           7                    []  \n",
       "...         ...         ...                   ...  \n",
       "6873          0           5                    []  \n",
       "6874          0           1                 [nan]  \n",
       "6875          0           5                    []  \n",
       "6876          0           6                    []  \n",
       "6877          0           1                    []  \n",
       "\n",
       "[6871 rows x 9 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "6181\n"
     ]
    }
   ],
   "source": [
    "# Создать Ndarray Numpy копию Series Pandas.\n",
    "y_trainNp = y_train.to_numpy()\n",
    "print(type(y_trainNp))\n",
    "print(y_trainNp.size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[list([]) list([nan])] \n",
      " unique.size:  2\n",
      "[4658 1523]\n"
     ]
    }
   ],
   "source": [
    "# Выделим уникальные значения y_train_np и посчитаем их количество.\n",
    "unique, counts = np.unique(y_trainNp, return_counts = True)\n",
    "print(unique, '\\n', \"unique.size: \", unique.size)\n",
    "print(counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AUC</th>\n",
       "      <th>F2</th>\n",
       "      <th>G2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>lstm_ICBEB_form</th>\n",
       "      <td>0.792765</td>\n",
       "      <td>0.498645</td>\n",
       "      <td>0.281553</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lstm_bidir_ICBEB_form</th>\n",
       "      <td>0.895261</td>\n",
       "      <td>0.747274</td>\n",
       "      <td>0.512097</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lstm_ptbxl_form</th>\n",
       "      <td>0.804124</td>\n",
       "      <td>0.041219</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lstm_bidir_ptbxl_form</th>\n",
       "      <td>0.802981</td>\n",
       "      <td>0.041219</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lstm_finetuning_form</th>\n",
       "      <td>0.691744</td>\n",
       "      <td>0.279402</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            AUC        F2        G2\n",
       "lstm_ICBEB_form        0.792765  0.498645  0.281553\n",
       "lstm_bidir_ICBEB_form  0.895261  0.747274  0.512097\n",
       "lstm_ptbxl_form        0.804124  0.041219  0.000000\n",
       "lstm_bidir_ptbxl_form  0.802981  0.041219  0.000000\n",
       "lstm_finetuning_form   0.691744  0.279402  0.000000"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "table = pd.read_csv('table_res_finetuning.csv', index_col=0)\n",
    "table"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "7def36fe8bd5fec4bb5af778b23511c8ac7b24717e5ef7201919a5b3f553865a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
